{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "\n",
    "# Homework 3:  Classification Cook-off: Naive Bayes vs Rocchio (plus a little bit of recommenders)\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Wednesday, March 29 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* Hands-on practice building and evaluating classifiers.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as YOUR_UIN_hw3.ipynb. Submit this notebook via eCampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Saturday April 1 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`# Part 0: Yelp review data\n",
    "\n",
    "In this assignment, given a Yelp review, your task is to implement two classifiers to predict if the business category of this review is \"food-relevant\" or not, **only based on the review text**. The data is from the [Yelp Dataset Challenge](https://www.yelp.com/dataset_challenge).\n",
    "\n",
    "## Build the training data\n",
    "\n",
    "First, you will need to download this data file as your training data: [training_data.json](https://drive.google.com/open?id=0B_13wIEAmbQMdzBVTndwenoxQlk) \n",
    "\n",
    "The training data file includes 40,000 Yelp reviews. Each line is a json-encoded review, and **you should only focus on the \"text\" field**. As same as in homework 1, you should tokenize the review text by using the regular expression \"\\W+\" (we discussed it in [this Piazza post](https://piazza.com/class/ixkk1fy863r1vs?cid=29). Do NOT remove stop words. **Do casefolding but no stemming**.\n",
    "\n",
    "The label (class) information of each review is in the \"label\" field. It is **either \"Food-relevant\" or \"Food-irrelevant\"**.\n",
    "\n",
    "## Testing data\n",
    "\n",
    "We provide 100 yelp reviews here: [testing_data.json](https://drive.google.com/open?id=0B_13wIEAmbQMbXdyTkhrZDN4Wms). The testing data file has the same format as the training data file. Again, you can get the label informaiton in the \"label\" field. Only use it when you evalute your classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# read the training data\n",
    "data = []\n",
    "data2 = []\n",
    "with open('training_data.json') as data_file:\n",
    "    for line in data_file: \n",
    "        data.append(json.loads(line))\n",
    "\n",
    "with open('testing_data.json') as data_file:\n",
    "    for line in data_file: \n",
    "        data2.append(json.loads(line))\n",
    "\n",
    "\n",
    "# print (data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "# tokenization \n",
    "def tokenizer(document):\n",
    "    document = document.lower()\n",
    "    wl = re.split('\\W+',document)\n",
    "    wl = filter(None, wl)\n",
    "    return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pair the tokenlist and label forming the training data\n",
    "label = []\n",
    "tokenList = []\n",
    "for review in data:\n",
    "    token = tokenizer(review[\"text\"])\n",
    "    tokenList.append(token)\n",
    "    label.append(review[\"label\"])\n",
    "\n",
    "t_data = zip(tokenList,label)\n",
    "\n",
    "# print len(tokenList[2])\n",
    "# print (label[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testing data\n",
    "label2 = []\n",
    "tokenList2 = []\n",
    "for review in data2:\n",
    "    token = tokenizer(review[\"text\"])\n",
    "    tokenList2.append(token)\n",
    "    label2.append(review[\"label\"])\n",
    "\n",
    "test_data = zip(tokenList2,label2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Naive Bayes classifier [35 points]\n",
    "\n",
    "In this part, you will implement a Naive Bayes classifier, which outputs the probabilities that a given review belongs to each class.\n",
    "\n",
    "Use a mixture model that mixes the probability from the document with the general collection frequency of the word. **You should use lambda = 0.7**. Be careful about the decimal rounding since multiplying many probabilities can generate a tiny value. We will not grade on the exact probability value, so feel free to change to logorithm summation (it's not required, though). If the tie case happens, **always go to the \"Food-irrelevant\" side**.\n",
    "\n",
    "### What to report\n",
    "\n",
    "* For the entire testing dataset, report the overall accuracy.\n",
    "* For the class \"Food-relevant\", report the precision and recall.\n",
    "* For the class \"Food-irrelevant\", report the precision and recall.\n",
    "\n",
    "We will also grade on the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step 1: get the probability of each term in vocabulary in traing_data\n",
    "# step 2: get the probability of each term in vocabulary of each calss, \n",
    "# step 3: use the formula (smoothin)pro_term_class = lamda * pro_term_class + (1-lamda) * prob_overall_term\n",
    "# step 4: get the prior of each class, \n",
    "# step 5: use log10(prior(class)) + sum(log10(pro(term|class)) to get the condition_prob(class|reviews)\n",
    "# step 6: assign the class with higher condion_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the naive bayes classifier\n",
    "# Insert as many cells as you want\n",
    "import collections\n",
    "bag_of_word = []\n",
    "r_bag_of_word = []\n",
    "i_bag_of_word = []\n",
    "i_number = 0\n",
    "r_number = 0\n",
    "# generate the bag of word and its vocabulary\n",
    "for entry in t_data:\n",
    "    bag_of_word += entry[0]\n",
    "voca = list(set(bag_of_word))\n",
    "\n",
    "# get the the number of entry and text corpus for each class, and relevant voca and irrelevant voca\n",
    "\n",
    "for text,c in t_data:\n",
    "    if(c == \"Food-relevant\"):\n",
    "        r_bag_of_word += text\n",
    "        r_number += 1\n",
    "    else:\n",
    "        i_bag_of_word += text\n",
    "        i_number += 1\n",
    "\n",
    "r_voca = list(set(r_bag_of_word))\n",
    "i_voca = list(set(i_bag_of_word))\n",
    "\n",
    "\n",
    "# print len(r_bag_of_word)\n",
    "# print len(i_bag_of_word)\n",
    "# print len(r_voca)\n",
    "# print len(i_voca)\n",
    "# print len(r_bag_of_word) + len(i_bag_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step 1: get the probability of each term in vocabulary in traing_data\n",
    "\n",
    "pro_term_whole = collections.defaultdict(float)\n",
    "w_term_fre = collections.Counter(bag_of_word)\n",
    "for term in voca:\n",
    "    pro_term_whole[term] = float(w_term_fre[term]) / len(bag_of_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step 2: get the probability of each term in vocabulary of each calss,\n",
    "# step 3: get the prior of each class, \n",
    "\n",
    "prob_i_term = collections.defaultdict(float)\n",
    "prob_r_term = collections.defaultdict(float)\n",
    "\n",
    "# step 2\n",
    "i_term_fre = collections.Counter(i_bag_of_word)\n",
    "r_term_fre = collections.Counter(r_bag_of_word)\n",
    "\n",
    "for term in voca:\n",
    "    prob_i_term[term] = float(i_term_fre[term]) / len(i_bag_of_word)\n",
    "    prob_r_term[term] = float(r_term_fre[term]) / len(r_bag_of_word)\n",
    "        \n",
    "# step 3    \n",
    "i_prior = float(i_number) / 40000\n",
    "r_prior = float(r_number) / 40000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply your classifier on the test data. Report the results.\n",
    "# Insert as many cells as you want\n",
    "import math\n",
    "\n",
    "# record the predictive class for testing data\n",
    "predict_c = []\n",
    "compare_list = []\n",
    "r_prob_dic = {}\n",
    "i_prob_list = {}\n",
    "\n",
    "for entry in test_data:\n",
    "    r_result = math.log10(r_prior)\n",
    "    i_result = math.log10(i_prior)\n",
    "#     entry_voca = lset(entry[0])\n",
    "#     actual_voca = entry_voca.difference(set(voca))\n",
    "# #   not in r_voca but in i_voca\n",
    "#     voca1 = actual_voca.difference\n",
    "    for term in entry[0]:\n",
    "        if (term in voca):\n",
    "            r_result += math.log10(0.7 * prob_r_term[term] + 0.3 * pro_term_whole[term])\n",
    "            i_result += math.log10(0.7 * prob_i_term[term] + 0.3 * pro_term_whole[term])\n",
    "    compare_list.append((r_result, i_result)) \n",
    "    \n",
    "    \n",
    "for i in range(len(test_data)):\n",
    "#   compare the the two result in c_list (for each entry)\n",
    "    if (compare_list[i][0] > compare_list[i][1] ):\n",
    "        predict_c.append(\"Food-relevant\")\n",
    "    else: \n",
    "        predict_c.append(\"Food-irrelevant\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the actual class list\n",
    "actual_c = []\n",
    "for entry in test_data:\n",
    "    actual_c.append(entry[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pair = zip(actual_c, predict_c)\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(actual_c)):\n",
    "    if (actual_c[i] == 'Food-relevant' and predict_c[i] == 'Food-relevant'):\n",
    "        TP += 1\n",
    "    elif (actual_c[i] == 'Food-irrelevant'and predict_c[i] == 'Food-relevant'):\n",
    "#         print i\n",
    "        FP += 1\n",
    "    elif (actual_c[i] == 'Food-relevant'and predict_c[i] == 'Food-irrelevant'):\n",
    "#         print i\n",
    "        FN += 1\n",
    "    else:\n",
    "        TN += 1\n",
    "             \n",
    "# print TP\n",
    "# print TN\n",
    "# print FP\n",
    "# print FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.89\n",
      "Food-relevant-precision is  0.901408450704\n",
      "Food-relevant-recall is  0.941176470588\n",
      "Food-irrelevant-precision is  0.862068965517\n",
      "Food-irrelevant-recall is  0.78125\n"
     ]
    }
   ],
   "source": [
    "accuracy = float(TP + TN) / len(test_data)\n",
    "Food_relevant_precision = float(TP) / (TP + FP)\n",
    "Food_relevant_recall = float(TP) / (TP + FN)\n",
    "Food_irrelevant_precision = float(TN) / (TN + FN)\n",
    "Food_irrelevant_recall = float(TN) / (TN + FP)\n",
    "\n",
    "print 'Accuracy is ',accuracy\n",
    "print 'Food-relevant-precision is ', Food_relevant_precision\n",
    "print 'Food-relevant-recall is ', Food_relevant_recall\n",
    "print 'Food-irrelevant-precision is ', Food_irrelevant_precision\n",
    "print 'Food-irrelevant-recall is ', Food_irrelevant_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.460435080864\n"
     ]
    }
   ],
   "source": [
    "f = 0.9014*0.9412/(0.9014+0.9412)\n",
    "print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Rocchio classifier [35 points]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, your job is to implement a Rocchio classifier for \"food-relevant vs. food-irrelevant\". You need to aggregate all the reviews of each class, and find the center. **Use the normalized raw term frequency**.\n",
    "\n",
    "\n",
    "### What to report\n",
    "\n",
    "* For the entire testing dataset, report the overall accuracy.\n",
    "* For the class \"Food-relevant\", report the precision and recall.\n",
    "* For the class \"Food-irrelevant\", report the precision and recall.\n",
    "\n",
    "We will also grade on the quality of your code. So make sure that your code is clear and readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step1:  divide the t_data by class\n",
    "# step2: calculate the term frequency dictionary for each entry in the t_data, then normalized:\n",
    "#         1.make each entry a dictionary,key is term and tf as value\n",
    "#         2.divide it by the length of the vector\n",
    "# step3: get the centroid of each calss:\n",
    "#         1.combine the dictionary for each class\n",
    "#         2.divide it by the number of entry in each class\n",
    "# step4: for test_data, get tf-dic for each entry and normalized\n",
    "# step5: meaure the distance between each class's centroid and each entry's tf-dic\n",
    "#        1. for each class's centroid, get the union voca of each test_data's entry's voca and centroid voca\n",
    "#        2. for each term in this union voca, measure euclidean distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Build the Rocchio classifier\n",
    "# Insert as many cells as you want\n",
    "\n",
    "# step1: divde the training data by class\n",
    "r_entryList = []\n",
    "i_entryList = []\n",
    "\n",
    "for entry in t_data:\n",
    "        if (entry[1] == \"Food-relevant\"):\n",
    "            r_entryList.append(entry[0])\n",
    "        else:\n",
    "            i_entryList.append(entry[0])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step2: calculate the term frequency dictionary for each entry in the t_data, then normalized:\n",
    "#         1.make each entry in each class a dictionary,key is term and tf as value\n",
    "#         2.divide it by the length of the vector\n",
    "\n",
    "# helpe method to get the length of the vector represented by dictionary\n",
    "def vector_len(dic):\n",
    "    value_list = dic.values()\n",
    "    length = math.sqrt(sum(a * a for a in value_list))\n",
    "    return length\n",
    "\n",
    "def norm_tf_list (entryList):\n",
    "    tf_dic_list = []\n",
    "    for entry in entryList:\n",
    "        tf_dic= collections.Counter(entry)\n",
    "        length = vector_len(tf_dic)\n",
    "        tf_norm = {k: (v / length) for k, v in tf_dic.iteritems()}\n",
    "#       append the dictionary to the tf_dic_list\n",
    "        tf_dic_list.append(tf_norm)\n",
    "    return tf_dic_list\n",
    "\n",
    "r_tf_list = norm_tf_list (r_entryList)\n",
    "i_tf_list = norm_tf_list (i_entryList)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step3: get the centroid of each calss:\n",
    "#         1.go through each entry dictionary, if exist, count+current, else, create key and count = current\n",
    "#         2.divide it by the number of entry in each class\n",
    "\n",
    "def centroid(tf_list,voca):\n",
    "    dic ={}\n",
    "#   initial the dic\n",
    "    for term in voca:\n",
    "        dic[term] = 0\n",
    "#   get the number of entry in each class\n",
    "    N = len(tf_list)\n",
    "#   go through each entry dictionary, dic[key]+v\n",
    "    for i in range(N):\n",
    "        for k,v in tf_list[i].items():\n",
    "            dic[k] += v\n",
    "#   divide it by the number of entry in each class\n",
    "    dic = {k: v / N for k,v in dic.items()}\n",
    "    return dic\n",
    "\n",
    "r_centroid = centroid(r_tf_list, r_voca)\n",
    "i_centroid = centroid(i_tf_list, i_voca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step4: for test_data, get tf-dic for each entry and normalized like before\n",
    "\n",
    "test_entrylist  = []\n",
    "for item in test_data:\n",
    "    test_entrylist.append(item[0])\n",
    "\n",
    "test_dic_list = norm_tf_list(test_entrylist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# step5: meaure the distance between each class's centroid and each entry's tf-dic\n",
    "#        1. for each class's centroid, get the union voca of each test_data's entry's voca and centroid voca\n",
    "#        2. for each term in this union voca, measure euclidean distance\n",
    "\n",
    "# euclidean distant function\n",
    "def eu_dis(centroid, query):\n",
    "    c_voca = centroid.keys()\n",
    "    q_voca = query.keys()\n",
    "\n",
    "#   in c not in q\n",
    "    voca1 = set(c_voca).difference(set(q_voca))\n",
    "#   in q not in c\n",
    "    voca2 = set(q_voca).difference(set(c_voca))\n",
    "#   in c and in q\n",
    "    voca3 = set(c_voca).intersection(set(q_voca))\n",
    "    \n",
    "    s = 0\n",
    "    for term in voca3:\n",
    "        s += (centroid[term] - query[term])**2\n",
    "    for term in voca2:\n",
    "        s +=  query[term] ** 2\n",
    "    for term in voca1:\n",
    "        s +=  centroid[term] ** 2\n",
    "    \n",
    "    dis = math.sqrt(s)       \n",
    "    return dis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply your classifier on the test data. Report the results.\n",
    "# Insert as many cells as you want\n",
    "r_predict_c = []\n",
    "for query in test_dic_list:\n",
    "    r_dis = eu_dis(r_centroid,query)\n",
    "    i_dis = eu_dis(i_centroid,query)\n",
    "    if (r_dis < i_dis):\n",
    "        r_predict_c.append(\"Food-relevant\")\n",
    "    else:\n",
    "        r_predict_c.append(\"Food-irrelevant\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pair = zip(actual_c, r_predict_c)\n",
    "TP = 0\n",
    "TN = 0\n",
    "FP = 0\n",
    "FN = 0\n",
    "\n",
    "for i in range(len(actual_c)):\n",
    "    if (actual_c[i] == 'Food-relevant' and r_predict_c[i] == 'Food-relevant'):\n",
    "        TP += 1\n",
    "    elif (actual_c[i] == 'Food-irrelevant'and r_predict_c[i] == 'Food-relevant'):\n",
    "#         print i\n",
    "        FP += 1\n",
    "    elif (actual_c[i] == 'Food-relevant'and r_predict_c[i] == 'Food-irrelevant'):\n",
    "#         print i\n",
    "        FN += 1\n",
    "    else:\n",
    "        TN += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.65\n",
      "Food-relevant-precision is  0.8\n",
      "Food-relevant-recall is  0.647058823529\n",
      "Food-irrelevant-precision is  0.466666666667\n",
      "Food-irrelevant-recall is  0.65625\n"
     ]
    }
   ],
   "source": [
    "accuracy = float(TP + TN) / len(test_data)\n",
    "Food_relevant_precision = float(TP) / (TP + FP)\n",
    "Food_relevant_recall = float(TP) / (TP + FN)\n",
    "Food_irrelevant_precision = float(TN) / (TN + FN)\n",
    "Food_irrelevant_recall = float(TN) / (TN + FP)\n",
    "\n",
    "print 'Accuracy is ',accuracy\n",
    "print 'Food-relevant-precision is ', Food_relevant_precision\n",
    "print 'Food-relevant-recall is ', Food_relevant_recall\n",
    "print 'Food-irrelevant-precision is ', Food_irrelevant_precision\n",
    "print 'Food-irrelevant-recall is ', Food_irrelevant_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.357738106427\n"
     ]
    }
   ],
   "source": [
    "f = 0.8 * 0.6470588/(0.8 + 0.647)\n",
    "print f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 3: Naive Bayes vs. Rocchio [20 points]\n",
    "\n",
    "Which method gives the better results? In terms of what? How did you compare them? Can you explain why you observe what you do? Write 1-3 paragraphs below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add your answer here:**\n",
    "1. Based on the result, we can see that the accuracy of Naive Bayes is about more than that of Rocchio. Moreover, the F score of Naive Bayes is about 0.47 while the one of Rocchio is 0.578.\n",
    "2. The reason i think is that naive bayes consider the probability of each word in the document while the Rocchio use a vector to represent the document. In Rocchio method, calculating the centroid of two classses may loss the latent characteristic of some documents. Meanwhile, the naive bayes method capture the every term's probability and also a global term probability. That's why Naive Bayes out perform the Rocchio method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Recommenders [10 points]\n",
    "\n",
    "Finally, since we've begun our discussion of recommenders, let's do a quick problem too:\n",
    "\n",
    "The table below is a utility matrix, representing the ratings, on a 1â€“5 star scale, of eight items, *a* through *h*, by three users *A*, *B*, and *C*. \n",
    "<pre>\n",
    "\n",
    "  | a  b  c  d  e  f  g  h\n",
    "--|-----------------------\n",
    "A | 4  5     5  1     3  2\n",
    "B |    3  4  3  1  2  1\n",
    "C | 2     1  3     4  5  3\n",
    "\n",
    "</pre>\n",
    "\n",
    "Compute the following from the data of this matrix.\n",
    "\n",
    "(a) Treating the utility matrix as boolean, compute the Jaccard distance between each pair of users.\n",
    "\n",
    "(b) Repeat Part (a), but use the cosine distance.\n",
    "\n",
    "(c) Treat ratings of 3, 4, and 5 as 1 and 1, 2, and blank as 0. Compute the Jaccard distance between each pair of users.\n",
    "\n",
    "(d) Repeat Part (c), but use the cosine distance.\n",
    "\n",
    "(e) Normalize the matrix by subtracting from each nonblank entry the average\n",
    "value for its user.\n",
    "\n",
    "(f) Using the normalized matrix from Part (e), compute the cosine distance\n",
    "between each pair of users.\n",
    "\n",
    "(g) Which of the approaches above seems most reasonable to you? Give a one or two sentence argument supporting your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add your answer here:**\n",
    "\n",
    "(a)Jac_dis(A,B) = 0.5  jac_dis(A,C) = 0.5, jac_dis(B,C)= 0.5\n",
    "\n",
    "(b)cos(A,B) = 0.67, cos(A,C) = 0.67, cos(B,C) = 0.67\n",
    "\n",
    "(c)Jac_dis(A,B) = 0.4  jac_dis(A,C) = 0.33, jac_dis(B,C)= 0.167\n",
    "<pre>\n",
    "\n",
    "  | a  b  c  d  e  f  g  h\n",
    "--|-----------------------\n",
    "A | 1  1  0  1  0  0  1  0\n",
    "B | 0  1  1  1  0  0  0  0\n",
    "C | 0  0  0  1  0  1  1  1\n",
    "\n",
    "</pre>\n",
    "\n",
    "\n",
    "(d)cos(A,B) = 0.58, cos(A,C) = 0.5, cos(B,C) = 0.29\n",
    "\n",
    "(e)\n",
    "<pre>\n",
    "  | a    b   c   d     e    f    g    h\n",
    "--|------------------------------------------\n",
    "A | 2/3 5/3     5/3  -7/3      -1/3  -4/3  \n",
    "B |     2/3 5/3 2/3  -4/3 -1/3 -4/3\n",
    "C | -1      -2   0          1    2    0\n",
    "</pre>\n",
    "\n",
    "(f)cos(A,B) = 0.58, cos(A,C) = -0.12, cos(B,C) = -0.74\n",
    "\n",
    "(g) I think the cosine similarity on the normalized matrix is most reasonable for me. As the normalized matrix make the consine similarity can reasonably calculate how they are similar to each other without the rating bias of the users. In contrast, the jaccard distance does not consider the rating itself and pure cosine similarity may ignore the higher and lower potention rating behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs670]",
   "language": "python",
   "name": "conda-env-cs670-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
