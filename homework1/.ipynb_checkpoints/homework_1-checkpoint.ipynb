{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSCE 670 :: Information Storage and Retrieval :: Texas A&M University :: Spring 2017\n",
    "\n",
    "\n",
    "# Homework 1:  Retrieval Models: Boolean + Vector Space\n",
    "\n",
    "### 100 points [5% of your final grade]\n",
    "\n",
    "### Due: Thursday, February 2 by 11:59pm\n",
    "\n",
    "*Goals of this homework:* In this homework you will get first hand experience building a text-based mini search engine. In particular, there are three main learning objectives: (i) the basics of tokenization (e.g. stemming, case-folding, etc.) and its effect on information retrieval; (ii) basics of index building and Boolean retrieval; and (iii) basics of the Vector Space model and ranked retrieval.\n",
    "\n",
    "*Submission Instructions:* To submit your homework, rename this notebook as lastname_firstinitial_hw#.ipynb. For example, my homework submission would be: caverlee_j_hw1.ipynb. Submit this notebook via ecampus. Your notebook should be completely self-contained, with the results visible in the notebook. \n",
    "\n",
    "*Late submission policy:* For this homework, you may use up to three of your late days, meaning that no submissions will be accepted after Friday, February 5 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We provide the dataset, [southpark_scripts.zip](https://www.dropbox.com/s/6rzfsbn97s8vwof/southpark_scripts.zip), which includes scripts for episodes of the first twenty seasons of the TV show South Park. You will build an inverted index over these scripts where each episode should be treated as a single document. There are 277 episodes (documents) to index and search on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (20 points) Part 1: Parsing\n",
    "\n",
    "First, you should tokenize documents using **whitespaces and punctuations as delimiters** but do not remove stop words. Your parser needs to also provide the following two confgiuration options:\n",
    "* Case-folding\n",
    "* Stemming: use [nltk Porter stemmer](http://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter)\n",
    "\n",
    "Please note that you should stick to the stemming package listed above. Otherwise, given the same query, the results generated by your code can be different from others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemming, True or False: True\n",
      "casefolding, True or False: True\n"
     ]
    }
   ],
   "source": [
    "# configuration options\n",
    "def str_to_bool(s):\n",
    "    if s == 'True':\n",
    "         return True\n",
    "    elif s == 'False':\n",
    "         return False\n",
    "    else:\n",
    "         raise ValueError\n",
    "\n",
    "stemming = raw_input('stemming, True or False: ')\n",
    "use_stemming = str_to_bool(stemming)\n",
    "\n",
    "\n",
    "casefolding = raw_input('casefolding, True or False: ')\n",
    "use_casefolding = str_to_bool(casefolding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your parser function here. It will take the two option variables above as the parameters\n",
    "# add cells as needed to organize your code\n",
    "\n",
    "import re\n",
    "from nltk import stem\n",
    "\n",
    "def parser(use_casefolding, use_stemming, document):  #return the list of words of this document\n",
    "    stemmed = []\n",
    "    \n",
    "    #casefolding the document\n",
    "    if (use_casefolding):  \n",
    "        document = document.lower()\n",
    "\n",
    "    #split words by whitespace and punctuation\n",
    "    wl = re.findall(r\"[\\w]+\", document) \n",
    "    \n",
    "    #stemming\n",
    "    if(use_stemming):     \n",
    "        stemmer = stem.PorterStemmer()\n",
    "        for words in wl:\n",
    "            stemmed.append(stemmer.stem(words))\n",
    "        return stemmed\n",
    "    else:\n",
    "        return wl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17143\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "combine_list = []\n",
    "filelist = []  \n",
    "\n",
    "#readin the scripts \n",
    "for filename in os.listdir(path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filelist.append(filename)\n",
    "\n",
    "\n",
    "#parse the words and save in result\n",
    "for filename in filelist:\n",
    "    with open(filename, 'r') as f:\n",
    "        outfile = f.read()\n",
    "        combine_list += parser(use_casefolding,use_stemming, outfile)\n",
    "\n",
    "#caculate the size of dictionary\n",
    "complete_wl = list(set(combine_list))\n",
    "size_dict = len(complete_wl)        \n",
    "\n",
    "print size_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Once you have your parser working, you should report here the size of your dictionary under the four cases. That is, how many unique tokens do you have with stemming on and casefolding on? And so on. You should fill in the following\n",
    "\n",
    "* Stemming + Casefolding = 17143\n",
    "* Stemming + No Casefolding =  17368\n",
    "* No Stemming + Casefolding    = 23806\n",
    "* No Stemming + No Casefolding =  29550\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 2: Boolean Retrieval\n",
    "\n",
    "In this part you build an inverted index to support Boolean retrieval. We only require your index to  support AND queries. In other words, your index does not have to support OR, NOT, or parentheses. Also, we do not explicitly expect to see AND in queries, e.g., when we query **great again**, your search engine should treat it as **great** AND **again**.\n",
    "\n",
    "Example queries:\n",
    "* Rednecks\n",
    "* Troll Trace\n",
    "* Respect my authority\n",
    "* Respect my authoritah\n",
    "* Respected my authority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import collections \n",
    "\n",
    "wordlist = []\n",
    "docID = []\n",
    "\n",
    "#generate the wordlist and docID list for each document\n",
    "for i in range(len(filelist)):\n",
    "    with open(filelist[i], 'r') as f:\n",
    "        outfile = f.read()\n",
    "        current_wl = parser(use_casefolding, use_stemming, outfile)\n",
    "        wordlist += current_wl\n",
    "        docID += [i] * len(current_wl)\n",
    "\n",
    "index_pair = zip(wordlist,docID)\n",
    "\n",
    "#create the inverted index\n",
    "post_index = collections.defaultdict(set)\n",
    "for k,v in index_pair:\n",
    "    post_index[k].add(v)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Search: fun\n",
      "157\n"
     ]
    }
   ],
   "source": [
    "search_text = raw_input('Boolean Search: ')\n",
    "\n",
    "# search for the input using your index and print out ids of matching documents\n",
    "wl = list(set(parser(use_casefolding, use_stemming, search_text)))\n",
    "\n",
    "#take intersection of the query and inverted_index\n",
    "result = []\n",
    "for w in wl:\n",
    "    doc_ids = post_index[w]\n",
    "    if result == []:\n",
    "        for doc in doc_ids:\n",
    "            result.append(doc)\n",
    "    else:\n",
    "        result = [val for val in doc_ids if val in result]\n",
    "\n",
    "#print out the name of the script based on the index result\n",
    "result.sort()\n",
    "doc_name = []\n",
    "for id in result:\n",
    "    doc_name.append(filelist[id])\n",
    "            \n",
    "print len(doc_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "When we use stemming and casefolding, is the result different from the result when we do not use them? Do you find cases where you prefer stemming? Or not? Or cases where you prefer casefolding? Or Not? Write down your observations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Stemming and casefolding are good strategies to reduce the empty result.\n",
    "2. For specific key words or phrases, for example the â€˜authoritah', which only exists in two scripts,stemming is not preferred. Searching for general meaning of the words or phrases. For example, \"respect the authority\" and \"respect my authority\" have the same meaning, if the user just want to get the documents have the similar meaning, the stemming procedure is preferred.\n",
    "3. Searching for case sensitive words or phrase like \"RESPECT MY AUTHORITAY\" which only exists in a single script. If we use the casefolding precedure, then the outcome will have many scripts. Then in this case, the casefolding procedure is preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Phrase Queries\n",
    "\n",
    "Your search engine needs to also (optionally) support phrase queries of arbitrary length. Use quotes in a query to tell your search engine this is a phrase query. Again, we don't explicitly type AND in queries and never use OR, NOT, or parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boolean Search (Phrase Query:\n"
     ]
    }
   ],
   "source": [
    "search_text = raw_input('Boolean Search (Phrase Query:')\n",
    "# search for the input using your index and print out ids of matching documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (40 points) Part 3: Ranked Retrieval\n",
    "\n",
    "In this part, your job is to support queries over an index that you build. This time you will use the vector space model plus cosine similarity to rank documents.\n",
    "\n",
    "**TFIDF:** For the document vectors, use the standard TFIDF scores. That is, use the log-weighted term frequency $1+log(tf)$; and the log-weighted inverse document frequency $log(\\frac{N}{df})$. For the query vector, use simple weights (the raw term frequency). For example:\n",
    "* query: troll $\\rightarrow$ (1)\n",
    "* query: troll trace $\\rightarrow$ (1, 1)\n",
    "\n",
    "**Output:**\n",
    "For a given query, you should rank all the 277 documents but you only need to output the top-5 documents (i.e. document ids) plus the cosine score of each of these documents. For example:\n",
    "\n",
    "* result1 - score1\n",
    "* result2 - score2\n",
    "* result3 - score3\n",
    "* result4 - score4\n",
    "* result5 - score5\n",
    "\n",
    "You can additionally assume that your queries will contain at most three words. Be sure to normalize your vectors as part of the cosine calculation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "N = len(filelist)\n",
    "\n",
    "# weighted term frequency\n",
    "def wtermFre(term, doc_rwl):\n",
    "    term_fre = doc_rwl.count(term)\n",
    "    if term_fre == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (1 + math.log(term_fre))\n",
    "\n",
    "#term document frequency is the length of the terms' inverted index list\n",
    "def wdocFre(term, post_index):\n",
    "    docFre = len(post_index[term])\n",
    "    return (math.log(float(N)/docFre))\n",
    "\n",
    "#tf-idf function\n",
    "def tf_idf(term, doc_rwl, post_index):\n",
    "    return (wtermFre(term, doc_rwl) *  wdocFre(term, post_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# building the vector space with normalized tfidf score\n",
    "vector_space = dict.fromkeys(i for i in range(len(filelist)))\n",
    "\n",
    "for doc_id in range(len(filelist)):\n",
    "    with open(filelist[doc_id], 'r') as f:\n",
    "        outfile = f.read()\n",
    "        raw_wl = parser(use_casefolding, use_stemming, outfile)\n",
    "        set_wl = list(set(raw_wl))\n",
    "        \n",
    "        #calculate tfidf for each unique word in this document\n",
    "        score_l = []\n",
    "        for w in set_wl:\n",
    "            score_l.append(tf_idf(w,raw_wl,post_index))\n",
    "        \n",
    "        #nomalization\n",
    "        eu_dis = math.sqrt(sum(val*val for val in score_l))\n",
    "        normal_score_l = [val/eu_dis for val in score_l]\n",
    "        \n",
    "        \n",
    "        #assign the score dictionary to the vector space\n",
    "        vector_space[doc_id] = dict(zip(set_wl,normal_score_l))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranked Search:'This is fun'\n",
      "[('1207.txt', 0.013128624243368683), ('0205.txt', 0.013072416112532716), ('0507.txt', 0.012728740967219059), ('1806.txt', 0.012660554725835755), ('1213.txt', 0.01245192243201136)]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "search_text = raw_input('Ranked Search:')\n",
    "# search for the input and print the top 5 document ids along with their associated cosine scores.\n",
    "\n",
    "# Generate query index \n",
    "q_wl = parser(use_casefolding, use_stemming, search_text)\n",
    "q_swl = list(set(q_wl))\n",
    "q_tf = [q_wl.count(w) for w in q_wl]\n",
    "q_index = dict(zip(q_wl,q_tf))\n",
    "\n",
    "#normalize it\n",
    "q_dis = math.sqrt(sum(tf*tf for tf in q_index.values()))\n",
    "for w,val in q_index.items():\n",
    "    q_index[w] = val/q_dis\n",
    "\n",
    "\n",
    "#calculate similarity between query and each doc, and generate the score for each doc\n",
    "\n",
    "score_list = []\n",
    "for i in range(len(filelist)):\n",
    "    doc_vec = []\n",
    "    q_vec = []\n",
    "    for w in q_index.keys():\n",
    "        if vector_space[i].has_key(w):\n",
    "            doc_vec.append(vector_space[i][w])\n",
    "        else:\n",
    "            doc_vec.append(0)\n",
    "        q_vec.append(q_index[w])\n",
    "    \n",
    "    sim_score = sum([a*b for a,b in zip(q_vec,doc_vec)])\n",
    "\n",
    "    score_list.append(sim_score) \n",
    "\n",
    "result = sorted(zip(filelist,score_list),key=lambda x: x[1],reverse = True)\n",
    "\n",
    "print result[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = 'This is fun'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How we grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grader will randomly pick 5-10 queries to test your program. You are welcome to discuss the results returned by your search engine with others on Piazza."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:cs670]",
   "language": "python",
   "name": "conda-env-cs670-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
